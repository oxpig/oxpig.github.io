{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34de022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FIELD_START_RE = re.compile(r'^\\s*\\w+\\s*=')\n",
    "\n",
    "def fix_bib_commas(input_path, output_path):\n",
    "    corrected_lines = []\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        in_entry = False\n",
    "        entry_header = None\n",
    "        entry_body = []\n",
    "        closing_line = None\n",
    "\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "\n",
    "            # Detect start of an entry\n",
    "            if not in_entry and stripped.startswith(\"@\"):\n",
    "                in_entry = True\n",
    "                entry_header = line\n",
    "                entry_body = []\n",
    "                closing_line = None\n",
    "                continue\n",
    "\n",
    "            # Detect end of an entry\n",
    "            if in_entry and stripped == \"}\":\n",
    "                closing_line = line\n",
    "                # Process the entry body to fix commas\n",
    "                processed_entry = process_entry(entry_header, entry_body, closing_line)\n",
    "                corrected_lines.extend(processed_entry)\n",
    "\n",
    "                # Reset flags\n",
    "                in_entry = False\n",
    "                entry_header = None\n",
    "                entry_body = []\n",
    "                closing_line = None\n",
    "                continue\n",
    "\n",
    "            # Inside an entry: collect body lines\n",
    "            if in_entry:\n",
    "                entry_body.append(line)\n",
    "            else:\n",
    "                # Outside entries, copy lines as-is\n",
    "                corrected_lines.append(line)\n",
    "\n",
    "    # Write out corrected content\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(corrected_lines)\n",
    "\n",
    "\n",
    "def process_entry(header_line, body_lines, closing_line):\n",
    "    \"\"\"\n",
    "    body_lines: all lines between the @entry{... line and the final '}' line.\n",
    "    We will:\n",
    "      - Find lines that start fields.\n",
    "      - For each field block (from field_start[i] to field_start[i+1]-1):\n",
    "          * ensure last non-blank line ends with comma (except for last field).\n",
    "      - For the last field:\n",
    "          * ensure last non-blank line does NOT end with comma.\n",
    "    \"\"\"\n",
    "    # Find indices of lines where a new field starts\n",
    "    field_indices = []\n",
    "    for i, ln in enumerate(body_lines):\n",
    "        if FIELD_START_RE.match(ln.strip()):\n",
    "            field_indices.append(i)\n",
    "\n",
    "    # If no fields detected, just return untouched\n",
    "    if not field_indices:\n",
    "        return [header_line] + body_lines + [closing_line]\n",
    "\n",
    "    # Helper to find the last non-empty line index in a given range (inclusive)\n",
    "    def last_nonempty_idx(start, end):\n",
    "        idx = end\n",
    "        while idx >= start and body_lines[idx].strip() == \"\":\n",
    "            idx -= 1\n",
    "        return idx if idx >= start else None\n",
    "\n",
    "    # Ensure commas for all fields except last\n",
    "    for k in range(len(field_indices) - 1):\n",
    "        start = field_indices[k]\n",
    "        next_start = field_indices[k + 1]\n",
    "        end = next_start - 1\n",
    "\n",
    "        last_idx = last_nonempty_idx(start, end)\n",
    "        if last_idx is None:\n",
    "            continue  # nothing non-empty in this block\n",
    "\n",
    "        line = body_lines[last_idx].rstrip(\"\\n\")\n",
    "        # Add comma only if it isn't already there\n",
    "        if not line.rstrip().endswith(\",\"):\n",
    "            line = line.rstrip() + \",\"\n",
    "        body_lines[last_idx] = line + \"\\n\"\n",
    "\n",
    "    # Handle last field: ensure NO trailing comma\n",
    "    last_field_start = field_indices[-1]\n",
    "    last_field_end = len(body_lines) - 1\n",
    "\n",
    "    last_idx = last_nonempty_idx(last_field_start, last_field_end)\n",
    "    if last_idx is not None:\n",
    "        line = body_lines[last_idx].rstrip(\"\\n\")\n",
    "        stripped = line.rstrip()\n",
    "        if stripped.endswith(\",\"):\n",
    "            stripped = stripped[:-1]  # remove the final comma\n",
    "        body_lines[last_idx] = stripped + \"\\n\"\n",
    "\n",
    "    return [header_line] + body_lines + [closing_line]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea49b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_bib_commas(\"/home/haque/Downloads/papers.bib\", \"/home/haque/Downloads/output.bib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb42926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Image Filename Generator (Your Required Rule)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def generate_image_filename(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate filename: first letter of first name + last name, all lower case.\n",
    "    Example: 'Oliver Turnbull' -> 'oturnbull.jpg'\n",
    "    \"\"\"\n",
    "    parts = name.strip().split()\n",
    "    if len(parts) == 0:\n",
    "        return \"default.jpg\"\n",
    "\n",
    "    first = parts[0]\n",
    "    last = parts[-1]  # last token is last name\n",
    "\n",
    "    filename = (first[0] + last).lower()\n",
    "    filename = re.sub(r\"[^a-z0-9]\", \"\", filename)  # remove punctuation\n",
    "\n",
    "    return f\"{filename}.jpg\"\n",
    "\n",
    "\n",
    "def slugify_name(name: str) -> str:\n",
    "    \"\"\"Convert a name to a slug for use in about_<slug>.md filenames.\"\"\"\n",
    "    name = name.strip().lower()\n",
    "    name = re.sub(r\"[^a-z0-9]+\", \"-\", name)\n",
    "    name = re.sub(r\"-+\", \"-\", name)\n",
    "    return name.strip(\"-\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Parsing functions\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def parse_members_file(file_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse a members text file.\n",
    "    Keeps only current members (Left == '-' or blank).\n",
    "    Adds derived fields including image filename and slug.\n",
    "    \"\"\"\n",
    "    members = []\n",
    "    with Path(file_path).open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader, None)  # skip header\n",
    "\n",
    "        for row in reader:\n",
    "            if not row or not row[0].strip():\n",
    "                continue\n",
    "\n",
    "            # Guarantee at least 6 columns\n",
    "            row += [\"\"] * (6 - len(row))\n",
    "\n",
    "            name = row[0].strip()\n",
    "            position = row[1].strip()\n",
    "            research = row[2].strip()\n",
    "            link = row[3].strip()\n",
    "            arrived = row[4].strip()\n",
    "            left = row[5].strip()\n",
    "\n",
    "            # Skip former members\n",
    "            if left not in (\"\", \"-\"):\n",
    "                continue\n",
    "\n",
    "            members.append({\n",
    "                \"name\": name,\n",
    "                \"position\": position,\n",
    "                \"research\": research,\n",
    "                \"link\": link,\n",
    "                \"arrived\": arrived,\n",
    "                \"image\": generate_image_filename(name),   # << ADDED\n",
    "                \"slug\": slugify_name(name)                # << ADDED\n",
    "            })\n",
    "\n",
    "    return members\n",
    "\n",
    "\n",
    "def merge_members(file_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Merge multiple member files without duplicates.\"\"\"\n",
    "    seen = set()\n",
    "    merged = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        for m in parse_members_file(file):\n",
    "            key = (m[\"name\"], m[\"position\"], m[\"arrived\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                merged.append(m)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Markdown Generation\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def generate_profiles_markdown(members: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Return Markdown text for profiles.md.\"\"\"\n",
    "\n",
    "    def sort_key(m):\n",
    "        try:\n",
    "            yr = int(m[\"arrived\"])\n",
    "        except:\n",
    "            yr = 9999\n",
    "        return (yr, m[\"name\"])\n",
    "\n",
    "    members_sorted = sorted(members, key=sort_key)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"---\\n\")\n",
    "    lines.append(\"layout: profiles\\n\")\n",
    "    lines.append(\"permalink: /people/\\n\")\n",
    "    lines.append(\"title: people\\n\")\n",
    "    lines.append(\"description: members of the lab or group\\n\")\n",
    "    lines.append(\"nav: true\\n\")\n",
    "    lines.append(\"nav_order: 7\\n\\n\")\n",
    "    lines.append(\"profiles:\\n\")\n",
    "\n",
    "    for i, m in enumerate(members_sorted):\n",
    "        align = \"left\" if i % 2 == 0 else \"right\"\n",
    "\n",
    "        lines.append(f\"  - align: {align}\\n\")\n",
    "        lines.append(f\"    image: {m['image']}\\n\")\n",
    "        lines.append(f\"    content: about_{m['slug']}.md\\n\")\n",
    "        lines.append(\"    image_circular: true\\n\")\n",
    "        lines.append(\"    more_info: >\\n\")\n",
    "        lines.append(f\"      <p>{m['position']}</p>\\n\")\n",
    "\n",
    "        if m[\"research\"] and m[\"research\"] != \"#\":\n",
    "            lines.append(f\"      <p>Research area: {m['research']}</p>\\n\")\n",
    "\n",
    "        if m[\"link\"] and m[\"link\"] != \"#\":\n",
    "            lines.append(f\"      <p><a href=\\\"{m['link']}\\\">{m['link']}</a></p>\\n\")\n",
    "\n",
    "        if m[\"arrived\"]:\n",
    "            lines.append(f\"      <p>Member since {m['arrived']}</p>\\n\")\n",
    "\n",
    "    lines.append(\"---\\n\")\n",
    "    return \"\".join(lines)\n",
    "\n",
    "\n",
    "def write_profiles_md(output_path: str, markdown_text: str):\n",
    "    Path(output_path).write_text(markdown_text, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# List your member files (you can add more)\n",
    "files = [\n",
    "    \"/home/haque/Downloads/members_all.txt\",\n",
    "    \"/home/haque/Downloads/members_visitor.txt\",\n",
    "    # \"members_extra.txt\",\n",
    "]\n",
    "\n",
    "members = merge_members(files)\n",
    "\n",
    "md_text = generate_profiles_markdown(members)\n",
    "\n",
    "write_profiles_md(\"profiles.md\", md_text)\n",
    "\n",
    "md_text  # display in notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
